{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "#ScDeepSort Imports\n",
    "from dance.modules.single_modality.cell_type_annotation.scdeepsort import ScDeepSort\n",
    "from dance.utils import set_seed\n",
    "\n",
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "from pprint import pprint\n",
    "from dance.datasets.singlemodality import ScDeepSortDataset\n",
    "\n",
    "import scanpy as sc\n",
    "from dance.transforms import AnnDataTransform, FilterGenesPercentile\n",
    "from dance.transforms import Compose, SetConfig\n",
    "from dance.transforms.graph import PCACellFeatureGraph, CellFeatureGraph\n",
    "from dance.typing import LogLevel, Optional\n",
    "\n",
    "from data_pre import data_pre\n",
    "from WordSage import WordSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print(x)\n",
    "else:\n",
    "    print (\"GPU not found.\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, z_dim=8):\n",
    "        super(BetaVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, z_dim*2),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 21)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        distributions = self.encoder(x)\n",
    "        x_map = F.softmax(distributions[:, :self.z_dim])\n",
    "        #std = F.softplus(distributions[:, self.z_dim:])\n",
    "        y_map = F.softmax(distributions[:, self.z_dim:])\n",
    "        #z = self.reparametrize(mu, std)\n",
    "        z = z = (x_map + y_map) / 2\n",
    "        logit = self.decoder(z)\n",
    "        return logit, x_map, y_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 3\n",
    "model1 = BetaVAE().to(device)\n",
    "model2 = BetaVAE().to(device)\n",
    "model3 = BetaVAE().to(device)\n",
    "models = BetaVAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.apply(init_weights)\n",
    "model2.apply(init_weights)\n",
    "model3.apply(init_weights)\n",
    "models.apply(init_weights)\n",
    "model_e = models.encoder\n",
    "model_g = models.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2023-10-05 14:04:49,072][dance][set_seed] Setting global random seed to 42\n",
      "c:\\Users\\seand\\python\\dance\\WordSage.py:148: UserWarning: you are shuffling a 'Tensor' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(targets)\n"
     ]
    }
   ],
   "source": [
    "data = data_pre()\n",
    "in_channels = 100\n",
    "hidden_channels = 100\n",
    "out_channels = 100\n",
    "num_classes = 21\n",
    "WordSage = WordSAGE(in_channels, hidden_channels, out_channels, num_classes)\n",
    "inputs, targets, genes, normalized_raw_data, test, y_test = data.read_w2v()\n",
    "seed=42\n",
    "set_seed(42)\n",
    "encoding = np.hstack([targets, y_test])\n",
    "label_encoder = LabelEncoder().fit(encoding)\n",
    "targets_encoded = label_encoder.transform(targets)\n",
    "targets_encoded = torch.tensor(targets_encoded, dtype=torch.long).to(device)\n",
    "num_classes = max(targets_encoded)+1\n",
    "targets_encoded = F.one_hot(targets_encoded, num_classes=num_classes)\n",
    "test_encoded = label_encoder.transform(y_test)\n",
    "test_encoded = torch.tensor(test_encoded, dtype=torch.long).to(device)\n",
    "test_encoded = F.one_hot(test_encoded, num_classes=num_classes)\n",
    "train_inputs, train_targets = WordSAGE.mix_data(self='', seed=seed, inputs=inputs, targets=targets_encoded)\n",
    "test_inputs, test_targets = WordSAGE.mix_data(self='', seed=seed, inputs=test, targets=test_encoded)\n",
    "train_inputs = torch.tensor(train_inputs, dtype=torch.float32).to(device)\n",
    "test_inputs = torch.tensor(test_inputs, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 ,X2, X3 = np.array_split(train_inputs, num_clients)\n",
    "y1, y2, y3 = np.array_split(train_targets, num_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\3391704767.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  d1 = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X1), torch.tensor(y1).long()), batch_size=32, shuffle=True)\n",
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\3391704767.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  d2 = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X2), torch.tensor(y2).long()), batch_size=32, shuffle=True)\n",
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\3391704767.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  d3 = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X3), torch.tensor(y3).long()), batch_size=32, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "d1 = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X1), torch.tensor(y1).long()), batch_size=32, shuffle=True)\n",
    "d2 = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X2), torch.tensor(y2).long()), batch_size=32, shuffle=True)\n",
    "d3 = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X3), torch.tensor(y3).long()), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1 = torch.optim.SGD(model1.parameters(), lr=1e-3, momentum=0.9)\n",
    "o2 = torch.optim.SGD(model2.parameters(), lr=1e-3, momentum=0.9)\n",
    "o3 = torch.optim.SGD(model3.parameters(), lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optim, data, verbose, alpha=0.1, beta=1e-3, lambda_=0.5):\n",
    "    cl_loss, acc = 0, 0\n",
    "    for inputs, targets in data:\n",
    "        logits, mu, std = model(inputs)\n",
    "\n",
    "        target_labels = torch.argmax(targets, dim=1)\n",
    "        class_loss = F.cross_entropy(logits, target_labels).div(math.log(2))\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        prediction = probs.max(1)[1]\n",
    "        accuracy = torch.eq(prediction, target_labels).float().mean()\n",
    "\n",
    "        cl_loss += class_loss.item()\n",
    "        acc += accuracy.item()\n",
    "    else:\n",
    "        if verbose:\n",
    "            cl_loss /= len(data)\n",
    "            acc /= len(data)\n",
    "            print(f'Epoch [{str(epoch).zfill(3)}], Class loss:{cl_loss:.4f}, Acc. {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\2850279677.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_map = F.softmax(distributions[:, :self.z_dim])\n",
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\2850279677.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_map = F.softmax(distributions[:, self.z_dim:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Class loss:4.8190, Acc. 0.06%\n",
      "Epoch [200], Class loss:4.8220, Acc. 0.06%\n",
      "Epoch [300], Class loss:4.8200, Acc. 0.06%\n"
     ]
    }
   ],
   "source": [
    "for e in range(300):\n",
    "    train(e+1, model1, o1, d1, (e+1) % 100 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\2850279677.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_map = F.softmax(distributions[:, :self.z_dim])\n",
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\2850279677.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_map = F.softmax(distributions[:, self.z_dim:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Class loss:4.4486, Acc. 1.87%\n",
      "Epoch [200], Class loss:4.4479, Acc. 1.89%\n",
      "Epoch [300], Class loss:4.4480, Acc. 1.85%\n"
     ]
    }
   ],
   "source": [
    "for e in range(300):\n",
    "    train(e+1, model2, o2, d2, (e+1) % 100 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\2850279677.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_map = F.softmax(distributions[:, :self.z_dim])\n",
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\2850279677.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_map = F.softmax(distributions[:, self.z_dim:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Class loss:4.5734, Acc. 0.45%\n",
      "Epoch [200], Class loss:4.5737, Acc. 0.45%\n",
      "Epoch [300], Class loss:4.5732, Acc. 0.45%\n"
     ]
    }
   ],
   "source": [
    "for e in range(300):\n",
    "    train(e+1, model3, o3, d3, (e+1) % 100 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\3278552767.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(test_inputs), torch.tensor(test_targets).long()), batch_size=128)\n"
     ]
    }
   ],
   "source": [
    "tl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(test_inputs), torch.tensor(test_targets).long()), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    acc = 0\n",
    "    for inputs, targets in data:\n",
    "        logits = model(inputs.float())[0]\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        prediction = probs.max(1)[1]\n",
    "        target_labels = torch.argmax(targets, dim=1)\n",
    "        accuracy = torch.eq(prediction, target_labels).float().mean()\n",
    "        acc += accuracy.item()\n",
    "    else:\n",
    "        acc /= len(data)\n",
    "        print(f'Acc. {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. 86.94%\n",
      "Acc. 0.00%\n",
      "Acc. 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\2850279677.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_map = F.softmax(distributions[:, :self.z_dim])\n",
      "C:\\Users\\seand\\AppData\\Local\\Temp\\ipykernel_3288\\2850279677.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_map = F.softmax(distributions[:, self.z_dim:])\n"
     ]
    }
   ],
   "source": [
    "test(model1, tl)\n",
    "test(model2, tl)\n",
    "test(model3, tl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
